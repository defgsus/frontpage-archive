# Archive of news front pages

[![Scraper](https://github.com/defgsus/frontpage-archive/actions/workflows/scraper.yml/badge.svg)](https://github.com/defgsus/teletext-archive/actions/workflows/scraper.yml)

Collector of raw index.html files and the like.
Should have started this 20 years ago! 


## Scraped sites:

%(table)s


Well, let's see how far this goes with a free github account. 
Many websites transmit click-ids and random uuids in their
documents so there is a change in every file in each snapshot. 

Anyways, currently each snapshot adds about 10mb to the
repository size (size of `.git` directory). That's not going
to work for long :-(

## UPDATE

Okay, raw data is just too much. The snapshot rate is now set
to **once a month**. I'll try to scrape just the article 
headlines and archive them in another repository.  


### TODO

- https://www.n-tv.de/
- https://www.handelsblatt.com/
- https://www.taz.de/
- https://www.wa.de/
- https://www.rnd.de/
- https://www.nzz.ch/
- https://www.bazonline.ch/
- https://www.focus.de/
- https://www.tagesschau.de/
- https://www.heise.de/tp/
- https://www.golem.de/
- https://www.kicker.de/
- https://www.achgut.com/
- https://www.stern.de/
